# Getting Started with the Data360 Pipeline Framework

## Introduction

The Data360 Pipeline Framework is designed to streamline the process of building and managing data pipelines. By using standardized functions and inputs, the framework reduces cognitive overload and enhances institutional knowledge, making it easier for staff to understand and maintain multiple pipelines. This chapter will explain the purpose of the framework, its main components, and provide a detailed description of each function used in a typical pipeline.

## Purpose of the Framework

The primary purpose of the Data360 Pipeline Framework is to provide a consistent and efficient way to build data pipelines. By standardizing the structure and components of pipelines, the framework ensures that all pipelines are built in a similar fashion, making them easier to understand, maintain, and extend. This consistency also facilitates knowledge transfer and collaboration among team members.

## Main Components

1. **Nodes**: The basic building blocks of a pipeline. Each node represents a single step in the data processing workflow.
2. **Pipelines**: A collection of nodes arranged in a specific order to achieve a desired data processing outcome.
3. **Catalog**: A configuration file that defines the datasets used in the pipeline, including their sources and destinations.
4. **Parameters**: Configuration settings that control the behavior of nodes and pipelines. These are typically defined in a parameters file.

## Description of Functions

Below is a description of each function used in a typical pipeline, including whether it is a standard function or a custom function, and its role in the overall pipeline.

### Custom Functions

1. **fetch_raw_data**
   - **Purpose**: Fetches raw data from a specified source.
   - **Inputs**: `params:wb_wdi.download_link` (URL to download the data)
   - **Outputs**: `WB_WDI_RAW` (Raw data)
   - **Role**: This function is responsible for downloading the raw data from the specified URL and storing it in the pipeline for further processing.

2. **reshape_raw_data**
   - **Purpose**: Reshapes the raw data into a format suitable for further processing.
   - **Inputs**: `JRC_EDGAR_RAW` (Raw data)
   - **Outputs**: `JRC_EDGAR_RAW_RESHAPED` (Reshaped data)
   - **Role**: This function transforms the raw data into a structured format, making it easier to work with in subsequent steps of the pipeline.

### Standard Functions

1. **fetch_dsd_schema**
   - **Purpose**: Fetches the Data Structure Definition (DSD) schema from a specified URL.
   - **Inputs**: 
     - `fmr_url`: URL to fetch the DSD schema
     - `dsd_id`: ID of the DSD schema
   - **Outputs**: `JRC_EDGAR_SCHEMA` (DSD schema)
   - **Role**: This function retrieves the schema that defines the structure of the data, ensuring that the data conforms to the expected format.

2. **validate_inputs**
   - **Purpose**: Validates the input data against predefined schemas.
   - **Inputs**: 
     - `inputs`: Data to be validated
     - `schemas`: Schemas to validate against
   - **Outputs**: `JRC_EDGAR_RAW_VALIDATION` (Validation results)
   - **Role**: This function checks the input data for conformity to the specified schemas, ensuring data quality and consistency.

3. **transform_source_to_target**
   - **Purpose**: Transforms the source data to the target format based on mapping rules.
   - **Inputs**: 
     - `data`: Source data
     - `mapping`: Mapping rules
   - **Outputs**: Transformed data
   - **Role**: This function applies the mapping rules to transform the source data into the target format, facilitating data integration and standardization.

4. **map_to_sdmx**
   - **Purpose**: Maps the transformed data to the SDMX (Statistical Data and Metadata eXchange) format.
   - **Inputs**: 
     - `data`: Transformed data
     - `mapping`: Mapping rules
   - **Outputs**: SDMX-formatted data
   - **Role**: This function ensures that the data conforms to the SDMX standard, enabling interoperability and data exchange.

5. **finalize_data_for_upload**
   - **Purpose**: Finalizes the data for upload to the target system.
   - **Inputs**: 
     - `data`: Data to be finalized
     - `dsd`: Data Structure Definition ID
   - **Outputs**: Finalized data
   - **Role**: This function prepares the data for upload, ensuring that it meets the requirements of the target system.

6. **partition_formatted_data**
   - **Purpose**: Partitions the formatted data into smaller chunks for easier processing and upload.
   - **Inputs**: Formatted data
   - **Outputs**: Partitioned data
   - **Role**: This function divides the data into smaller, more manageable chunks, facilitating efficient processing and upload.

## Example Pipeline

Below is an example of a pipeline built using the Data360 Pipeline Framework:


