# Data pipeline with FMR and pysdmx (draft)

## Introduction

Generic data pipeline flow:

1.  Fetch raw data (bronze)

2.  Tidy data (silver): Minor cleaning and turn into long / tidy format (one obeservation per row). This is a necessary step to allow standardization and scalability.

3.  Map data to target

4.  Validate (gold)

In order to make data pipelines run at scale, we need:

-   Standardized functions: Facilitate robustness and maintenance.

-   Decoupling of code and configuration: Separation of duty, easier maintenance

Solution:

Leverage SDMX ecosystem to implement this approach:

-   FMR to be used as the central repository to document configurations

    -   Tidy data documented using a dummy DSD / DF

    -   Selection of indicators, year, etc. through content constraint

    -   Mappings documented through structure maps

    -   Process flow documented through metadataflow

Below is and end-to-end implementation of this approach.

## TO DO

-   Automate the creation of dummy DSD from tidy data: What happens for non-existing concepts...? How to automatically generate codelists? (This may become crazy -\> Too many codelists...). Dots are not supported in indicator names, so even raw indicator codes would need to be modified.

-   

## Raw data

The primary purpose of the Data360 Pipeline Framework is to provide a consistent and efficient way to build data pipelines. By standardizing the structure and components of pipelines, the framework ensures that all pipelines are built in a similar fashion, making them easier to understand, maintain, and extend. This consistency also facilitates knowledge transfer and collaboration among team members.

## Main Components

1.  **Nodes**: The basic building blocks of a pipeline. Each node represents a single step in the data processing workflow.
2.  **Pipelines**: A collection of nodes arranged in a specific order to achieve a desired data processing outcome.
3.  **Catalog**: A configuration file that defines the datasets used in the pipeline, including their sources and destinations.
4.  **Parameters**: Configuration settings that control the behavior of nodes and pipelines. These are typically defined in a parameters file.

## Description of Functions

Below is a description of each function used in a typical pipeline, including whether it is a standard function or a custom function, and its role in the overall pipeline.

### Custom Functions

1.  **fetch_raw_data**
    -   **Purpose**: Fetches raw data from a specified source.
    -   **Inputs**: `params:wb_wdi.download_link` (URL to download the data)
    -   **Outputs**: `WB_WDI_RAW` (Raw data)
    -   **Role**: This function is responsible for downloading the raw data from the specified URL and storing it in the pipeline for further processing.
2.  **reshape_raw_data**
    -   **Purpose**: Reshapes the raw data into a format suitable for further processing.
    -   **Inputs**: `JRC_EDGAR_RAW` (Raw data)
    -   **Outputs**: `JRC_EDGAR_RAW_RESHAPED` (Reshaped data)
    -   **Role**: This function transforms the raw data into a structured format, making it easier to work with in subsequent steps of the pipeline.

### Standard Functions

1.  **fetch_dsd_schema**
    -   **Purpose**: Fetches the Data Structure Definition (DSD) schema from a specified URL.
    -   **Inputs**:
        -   `fmr_url`: URL to fetch the DSD schema
        -   `dsd_id`: ID of the DSD schema
    -   **Outputs**: `JRC_EDGAR_SCHEMA` (DSD schema)
    -   **Role**: This function retrieves the schema that defines the structure of the data, ensuring that the data conforms to the expected format.
2.  **validate_inputs**
    -   **Purpose**: Validates the input data against predefined schemas.
    -   **Inputs**:
        -   `inputs`: Data to be validated
        -   `schemas`: Schemas to validate against
    -   **Outputs**: `JRC_EDGAR_RAW_VALIDATION` (Validation results)
    -   **Role**: This function checks the input data for conformity to the specified schemas, ensuring data quality and consistency.
3.  **transform_source_to_target**
    -   **Purpose**: Transforms the source data to the target format based on mapping rules.
    -   **Inputs**:
        -   `data`: Source data
        -   `mapping`: Mapping rules
    -   **Outputs**: Transformed data
    -   **Role**: This function applies the mapping rules to transform the source data into the target format, facilitating data integration and standardization.
4.  **map_to_sdmx**
    -   **Purpose**: Maps the transformed data to the SDMX (Statistical Data and Metadata eXchange) format.
    -   **Inputs**:
        -   `data`: Transformed data
        -   `mapping`: Mapping rules
    -   **Outputs**: SDMX-formatted data
    -   **Role**: This function ensures that the data conforms to the SDMX standard, enabling interoperability and data exchange.
5.  **finalize_data_for_upload**
    -   **Purpose**: Finalizes the data for upload to the target system.
    -   **Inputs**:
        -   `data`: Data to be finalized
        -   `dsd`: Data Structure Definition ID
    -   **Outputs**: Finalized data
    -   **Role**: This function prepares the data for upload, ensuring that it meets the requirements of the target system.
6.  **partition_formatted_data**
    -   **Purpose**: Partitions the formatted data into smaller chunks for easier processing and upload.
    -   **Inputs**: Formatted data
    -   **Outputs**: Partitioned data
    -   **Role**: This function divides the data into smaller, more manageable chunks, facilitating efficient processing and upload.

## Example Pipeline

Below is an example of a pipeline built using the Data360 Pipeline Framework:
